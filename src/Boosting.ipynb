{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.data <- read.csv(file.path(\"..\", \"data\", \"training_data.csv\"))\n",
    "test.data <- read.csv(file.path(\"..\", \"data\", \"test_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "numeric.intensity <- as.numeric(train.data$Intensity)\n",
    "x <- train.data[, -c(2,3)]\n",
    "x$Intensity <- numeric.intensity\n",
    "n.before <- dim(x)[2] #numbers of predictors before the reduction\n",
    "idx.zero.var <- apply(x, 2, var) == 0\n",
    "x <- x[,!idx.zero.var]\n",
    "y <- train.data$VALENCE.PLEASANTNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for input variables with zero variance (without the varible Intensity)\n",
    "set.seed(100)\n",
    "x <- train.data[, -c(1,2,3)]\n",
    "\n",
    "idx.zero.var <- apply(x, 2, var) == 0\n",
    "x <- x[,!idx.zero.var]\n",
    "\n",
    "\n",
    "#attache Intensity as factor\n",
    "x$Intensity <- as.factor(train.data$Intensity)\n",
    "\n",
    "data <- x\n",
    "data$VALENCE.PLEASANTNESS <- train.data$VALENCE.PLEASANTNESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "#train and validation indexes\n",
    "len <- length(x[,1])\n",
    "idx.train <- sample(1:len, 2*len/3)\n",
    "\n",
    "#xgboost does not accept data frames therefore we will first convert the data into ordinary matrices\n",
    "library(xgboost)\n",
    "library(Matrix)\n",
    "train.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = data[idx.train,])\n",
    "validation.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = data[-idx.train,])\n",
    "train.y <- data$VALENCE.PLEASANTNESS[idx.train]\n",
    "validation.y <- data$VALENCE.PLEASANTNESS[-idx.train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.heart <- xgboost(train.x, label = train.y,\n",
    "                      objective = \"reg:squarederror\",\n",
    "                      eta = 0.01,\n",
    "                      max_depth = 2,\n",
    "                      nround = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.train <- predict(boost.heart, train.x)\n",
    "prediction.validation <- predict(boost.heart, validation.x)\n",
    "MSE.train <- mean((prediction.train - train.y)^2)\n",
    "MSE.validation <- mean((prediction.validation - validation.y)^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "300.145837255573"
      ],
      "text/latex": [
       "300.145837255573"
      ],
      "text/markdown": [
       "300.145837255573"
      ],
      "text/plain": [
       "[1] 300.1458"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "439.258313515555"
      ],
      "text/latex": [
       "439.258313515555"
      ],
      "text/markdown": [
       "439.258313515555"
      ],
      "text/plain": [
       "[1] 439.2583"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MSE.train\n",
    "MSE.validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "library(Matrix)\n",
    "#Boosting Submission\n",
    "set.seed(100)\n",
    "#Preparation of training and test data\n",
    "train <- train.data[, -c(1,2,3)]\n",
    "idx.zero.var <- apply(train, 2, var) == 0\n",
    "\n",
    "train <- train[,!idx.zero.var]\n",
    "test <- test.data[,-c(1,2)]\n",
    "test <- test[,!idx.zero.var]\n",
    "\n",
    "\n",
    "#test$Intensity <- as.factor(test.data$Intensity)\n",
    "train$Intensity <- as.numeric(train.data$Intensity)-1\n",
    "train\n",
    "#test intensity is always at level high, so that the prediction function has a problem (cheat with adding a row that afterwards is substracted)\n",
    "#test <- rbind(test, train[1,])\n",
    "train.x = train\n",
    "\n",
    "train$VALENCE.PLEASANTNESS <- train.data$VALENCE.PLEASANTNESS\n",
    "train.y = train$VALENCE.PLEASANTNESS\n",
    "\n",
    "train.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = train)\n",
    "#test.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = test)\n",
    "train.y <- train$VALENCE.PLEASANTNESS\n",
    "#validation.y <- data$VALENCE.PLEASANTNESS[-idx.train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test <- test.data[,-c(1,2)]\n",
    "test <- test[,!idx.zero.var]\n",
    "test$Intensity <- as.numeric(test.data$Intensity)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.heart <- xgboost(train.x, label = train.y,\n",
    "                      objective = \"reg:squarederror\",\n",
    "                      eta = 0.01,\n",
    "                      max_depth = 2,\n",
    "                      nround = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.boost = predict(boost.heart, as.matrix(test))\n",
    "submission <- data.frame(Id = 1:68, VALENCE.PLEASANTNESS = prediction.boost)\n",
    "write.csv(submission, file = \"../Submissions/boosting2.csv\", row.names = FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting 2 - Regularized gradient boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient (less splits and depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): objet 'x' introuvable\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): objet 'x' introuvable\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "set.seed(100)\n",
    "len <- length(x[,1])\n",
    "idx.train <- sample(1:len, 3*len/4)\n",
    "\n",
    "train.x <- x[idx.train,]\n",
    "train.y <- y[idx.train]\n",
    "test.x <- x[-idx.train,]\n",
    "test.y <- y[-idx.train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data =  as.matrix(train.x), label = train.y )\n",
    "dtest = xgb.DMatrix(data =  as.matrix(test.x), label = test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = list(train=dtrain, test=dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:36.493153\ttest-rmse:37.327053 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 50 rounds.\n",
      "\n",
      "Stopping. Best iteration:\n",
      "[7]\ttrain-rmse:10.149774\ttest-rmse:24.547579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bst = xgb.train(data = dtrain, \n",
    "                max.depth = 8, \n",
    "                eta = 0.3, \n",
    "                nthread = 2, \n",
    "                nround = 1000, \n",
    "                watchlist = watchlist, \n",
    "                objective = \"reg:squarederror\", \n",
    "                early_stopping_rounds = 50,\n",
    "                print_every_n = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a train-rmse of 10.15 and a test-rmse of 24.55. But, the parameters were chosen randomly\n",
    "Now, let's tune the algorithm with 3 parameters : \n",
    "1) The number of trees \n",
    "\n",
    "2) The shrinkage parameter lambda : Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small Î» can require using a very large value of B in order to achieve good performance.\n",
    "\n",
    "3) The number of splits in each tree, which controls the complexity of the boosted ensemble (controlled with max.depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a slower learning model, by reducing the learning rate, and reducing the number of splits in each tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:48.982822\ttest-rmse:47.107918 \n",
      "Multiple eval metrics are present. Will use test_rmse for early stopping.\n",
      "Will train until test_rmse hasn't improved in 50 rounds.\n",
      "\n",
      "Stopping. Best iteration:\n",
      "[280]\ttrain-rmse:12.274206\ttest-rmse:23.773617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bst_slow = xgb.train(data = dtrain, \n",
    "                        max.depth=5, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:squarederror\", \n",
    "                        early_stopping_rounds = 50,\n",
    "                        print_every_n = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we reduced the test-rmse by 6%. The problem here : What we have done here is fit to the training set and the test set at the same time (leading to model overfit). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We need to work with a validation set and only at the end evaluate the model performance against the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "len <- length(train.x[,1])\n",
    "idx.valid <- sample(1:len, 2*len/3)\n",
    "\n",
    "train.val.x <- train.x[idx.valid,]\n",
    "train.val.y <- train.y[idx.valid]\n",
    "\n",
    "valid.x <- train.x[-idx.valid,]\n",
    "valid.y <- train.y[-idx.valid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_train = xgb.DMatrix(data = as.matrix(train.val.x), label = train.val.y )\n",
    "gb_valid = xgb.DMatrix(data = as.matrix(valid.x), label = valid.y )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = list(train = gb_train, valid = gb_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:49.436710\tvalid-rmse:48.084187 \n",
      "Multiple eval metrics are present. Will use valid_rmse for early stopping.\n",
      "Will train until valid_rmse hasn't improved in 50 rounds.\n",
      "\n",
      "Stopping. Best iteration:\n",
      "[305]\ttrain-rmse:5.971423\tvalid-rmse:24.474022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bst_slow = xgb.train(data= gb_train, \n",
    "                        max.depth = 10, \n",
    "                        eta = 0.01, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:squarederror\", \n",
    "                        early_stopping_rounds = 50,\n",
    "                        print_every_n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "671.617282193548"
      ],
      "text/latex": [
       "671.617282193548"
      ],
      "text/markdown": [
       "671.617282193548"
      ],
      "text/plain": [
       "[1] 671.6173"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "25.9155799123529"
      ],
      "text/latex": [
       "25.9155799123529"
      ],
      "text/markdown": [
       "25.9155799123529"
      ],
      "text/plain": [
       "[1] 25.91558"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat_valid = predict(bst_slow, dtest)\n",
    "test_mse = mean(((y_hat_valid - test.y)^2))\n",
    "test_mse\n",
    "test_rmse = sqrt(test_mse)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is higher then on the first run, but we can be confident that the improved score is not due to overfit thanks to our use of a validation set! A lower rmse isn't necessarily better if it comes at the cose of overfit, we now have more confidence in external predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the best hyper-parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$max_depth</dt>\n",
       "\t\t<dd>7</dd>\n",
       "\t<dt>$eta</dt>\n",
       "\t\t<dd>0.001</dd>\n",
       "\t<dt>$nthread</dt>\n",
       "\t\t<dd>2</dd>\n",
       "\t<dt>$objective</dt>\n",
       "\t\t<dd>'reg:squarederror'</dd>\n",
       "\t<dt>$validate_parameters</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$max\\_depth] 7\n",
       "\\item[\\$eta] 0.001\n",
       "\\item[\\$nthread] 2\n",
       "\\item[\\$objective] 'reg:squarederror'\n",
       "\\item[\\$validate\\_parameters] TRUE\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$max_depth\n",
       ":   7\n",
       "$eta\n",
       ":   0.001\n",
       "$nthread\n",
       ":   2\n",
       "$objective\n",
       ":   'reg:squarederror'\n",
       "$validate_parameters\n",
       ":   TRUE\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$max_depth\n",
       "[1] 7\n",
       "\n",
       "$eta\n",
       "[1] 0.001\n",
       "\n",
       "$nthread\n",
       "[1] 2\n",
       "\n",
       "$objective\n",
       "[1] \"reg:squarederror\"\n",
       "\n",
       "$validate_parameters\n",
       "[1] TRUE\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "23.791611"
      ],
      "text/latex": [
       "23.791611"
      ],
      "text/markdown": [
       "23.791611"
      ],
      "text/plain": [
       "[1] 23.79161"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max.depths = c(7, 9)\n",
    "etas = c(0.1,0.01, 0.001)\n",
    "\n",
    "best_params = 0\n",
    "best_score = 0\n",
    "\n",
    "count = 1\n",
    "\n",
    "for( depth in max.depths ) {\n",
    "    for(num in etas) {\n",
    "\n",
    "        bst_grid = xgb.train(data = gb_train, \n",
    "                                max.depth = depth, \n",
    "                                eta=num, \n",
    "                                nthread = 2, \n",
    "                                nround = 10000, \n",
    "                                watchlist = watchlist, \n",
    "                                objective = \"reg:squarederror\", \n",
    "                                early_stopping_rounds = 50, \n",
    "                                verbose=0)\n",
    "\n",
    "        if(count == 1){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "            count = count + 1\n",
    "            }\n",
    "        else if( bst_grid$best_score < best_score){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_params\n",
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain-rmse:49.839478\tvalid-rmse:48.421314 \n",
      "Multiple eval metrics are present. Will use valid_rmse for early stopping.\n",
      "Will train until valid_rmse hasn't improved in 50 rounds.\n",
      "\n",
      "[501]\ttrain-rmse:32.446823\tvalid-rmse:34.793262 \n",
      "[1001]\ttrain-rmse:21.792664\tvalid-rmse:27.858894 \n",
      "[1501]\ttrain-rmse:15.200837\tvalid-rmse:24.959738 \n",
      "[2001]\ttrain-rmse:11.097453\tvalid-rmse:23.966742 \n",
      "Stopping. Best iteration:\n",
      "[2400]\ttrain-rmse:8.857841\tvalid-rmse:23.791611\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bst_tuned = xgb.train( data = gb_train, \n",
    "                        max.depth = 7, \n",
    "                        eta = 0.001, \n",
    "                        nthread = 2, \n",
    "                        nround = 10000, \n",
    "                        watchlist = watchlist, \n",
    "                        objective = \"reg:squarederror\", \n",
    "                        early_stopping_rounds = 50,\n",
    "                        print_every_n = 500)\n",
    "\n",
    "y_hat_xgb_grid = predict(bst_tuned, dtest)\n",
    "\n",
    "test_mse = mean(((y_hat_xgb_grid - test.y)^2))\n",
    "test_rmse = sqrt(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "25.7060135472519"
      ],
      "text/latex": [
       "25.7060135472519"
      ],
      "text/markdown": [
       "25.7060135472519"
      ],
      "text/plain": [
       "[1] 25.70601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now maybe try to submit to Kaggle to see if we get better on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
