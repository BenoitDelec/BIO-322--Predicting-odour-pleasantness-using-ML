{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.data <- read.csv(file.path(\"..\", \"data\", \"training_data.csv\"))\n",
    "test.data <- read.csv(file.path(\"..\", \"data\", \"test_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "numeric.intensity <- as.numeric(train.data$Intensity)-1\n",
    "x <- train.data[, -c(2,3)]\n",
    "x$Intensity <- numeric.intensity\n",
    "\n",
    "idx.zero.var <- apply(x, 2, var) == 0\n",
    "x <- x[,!idx.zero.var]\n",
    "y <- train.data$VALENCE.PLEASANTNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for Cross-Validation and Predictions\n",
    "fit_and_evaluate <- function(fold, formula = VALENCE.PLEASANTNESS ~ ., eta = 0.1, max.depth = 7, nrounds = 100  ) {\n",
    "    boosting <- randomForest::randomForest(VALENCE.PLEASANTNESS ~ ., fold, mtry = mtry, ntree = ntree, maxnodes = maxnodes)\n",
    "    valid.set <- assessment(fold)\n",
    "    mean((valid.set$VALENCE.PLEASANTNESS - predict(random.Forest, valid.set))^2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "#train and validation indexes\n",
    "len <- length(x[,1])\n",
    "idx.train <- sample(1:len, 2*len/3)\n",
    "\n",
    "#xgboost does not accept data frames therefore we will first convert the data into ordinary matrices\n",
    "library(xgboost)\n",
    "library(Matrix)\n",
    "train.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = data[idx.train,])\n",
    "validation.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = data[-idx.train,])\n",
    "train.y <- data$VALENCE.PLEASANTNESS[idx.train]\n",
    "validation.y <- data$VALENCE.PLEASANTNESS[-idx.train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boosting Cross Validation \n",
    "library(tidymodels)\n",
    "set.seed(100)\n",
    "\n",
    "full.data <- x\n",
    "full.data$VALENCE.PLEASANTNESS <- y\n",
    "\n",
    "\n",
    "len <- length(x[,1])\n",
    "idx.train <- sample(1:len, 3*len/4)\n",
    "\n",
    "train.x <- as.matrix(x[idx.train,])\n",
    "train.y <- y[idx.train]\n",
    "val.x <- as.matrix(x[-idx.train,])\n",
    "val.y <- y[-idx.train]\n",
    "\n",
    "\n",
    "#parameters\n",
    "eta = seq(0.1,0.3, 0.05)\n",
    "seeds <- seq.int(100,120)\n",
    "max.depth = seq(5,9,1)\n",
    "nrounds = seq(50, 350,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in eta {\n",
    "    boost.heart <- xgboost(train.x, label = train.y,\n",
    "                      objective = \"reg:squarederror\",\n",
    "                      eta = i,\n",
    "                      max_depth = 5,\n",
    "                      nround = 100, \n",
    "                          )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.heart <- xgboost(train.x, label = train.y,\n",
    "                      objective = \"reg:squarederror\",\n",
    "                      eta = 0.01,\n",
    "                      max_depth = 2,\n",
    "                      nround = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.train <- predict(boost.heart, train.x)\n",
    "prediction.validation <- predict(boost.heart, validation.x)\n",
    "MSE.train <- mean((prediction.train - train.y)^2)\n",
    "MSE.validation <- mean((prediction.validation - validation.y)^2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "300.145837255573"
      ],
      "text/latex": [
       "300.145837255573"
      ],
      "text/markdown": [
       "300.145837255573"
      ],
      "text/plain": [
       "[1] 300.1458"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "439.258313515555"
      ],
      "text/latex": [
       "439.258313515555"
      ],
      "text/markdown": [
       "439.258313515555"
      ],
      "text/plain": [
       "[1] 439.2583"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MSE.train\n",
    "MSE.validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)\n",
    "library(Matrix)\n",
    "#Boosting Submission\n",
    "set.seed(100)\n",
    "#Preparation of training and test data\n",
    "train <- train.data[, -c(1,2,3)]\n",
    "idx.zero.var <- apply(train, 2, var) == 0\n",
    "\n",
    "train <- train[,!idx.zero.var]\n",
    "test <- test.data[,-c(1,2)]\n",
    "test <- test[,!idx.zero.var]\n",
    "\n",
    "\n",
    "#test$Intensity <- as.factor(test.data$Intensity)\n",
    "train$Intensity <- as.numeric(train.data$Intensity)-1\n",
    "train\n",
    "#test intensity is always at level high, so that the prediction function has a problem (cheat with adding a row that afterwards is substracted)\n",
    "#test <- rbind(test, train[1,])\n",
    "train.x = train\n",
    "\n",
    "train$VALENCE.PLEASANTNESS <- train.data$VALENCE.PLEASANTNESS\n",
    "train.y = train$VALENCE.PLEASANTNESS\n",
    "\n",
    "train.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = train)\n",
    "#test.x <- sparse.model.matrix(VALENCE.PLEASANTNESS ~ . -1, data = test)\n",
    "train.y <- train$VALENCE.PLEASANTNESS\n",
    "#validation.y <- data$VALENCE.PLEASANTNESS[-idx.train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test <- test.data[,-c(1,2)]\n",
    "test <- test[,!idx.zero.var]\n",
    "test$Intensity <- as.numeric(test.data$Intensity)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost.heart <- xgboost(train.x, label = train.y,\n",
    "                      objective = \"reg:squarederror\",\n",
    "                      eta = 0.01,\n",
    "                      max_depth = 2,\n",
    "                      nround = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.boost = predict(boost.heart, as.matrix(test))\n",
    "submission <- data.frame(Id = 1:68, VALENCE.PLEASANTNESS = prediction.boost)\n",
    "write.csv(submission, file = \"../Submissions/boosting2.csv\", row.names = FALSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting 2 - Regularized gradient boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller trees are typically sufficient (less splits and depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "len <- length(x[,1])\n",
    "idx.train <- sample(1:len, 2*len/3)\n",
    "\n",
    "train.x <- x[idx.train,]\n",
    "train.y <- y[idx.train]\n",
    "val.x <- x[-idx.train,]\n",
    "val.y <- y[-idx.train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(data =  as.matrix(train.x), label = train.y )\n",
    "dval = xgb.DMatrix(data =  as.matrix(val.x), label = val.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "watchlist = list(train=dtrain, val=dval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tune the algorithm with 3 parameters : \n",
    "1) The number of trees \n",
    "\n",
    "2) The shrinkage parameter lambda : Typical values are 0.01 or 0.001, and the right choice can depend on the problem. Very small Î» can require using a very large value of B in order to achieve good performance.\n",
    "\n",
    "3) The number of splits in each tree, which controls the complexity of the boosted ensemble (controlled with max.depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$max_depth</dt>\n",
       "\t\t<dd>6</dd>\n",
       "\t<dt>$eta</dt>\n",
       "\t\t<dd>0.05</dd>\n",
       "\t<dt>$objective</dt>\n",
       "\t\t<dd>'reg:squarederror'</dd>\n",
       "\t<dt>$validate_parameters</dt>\n",
       "\t\t<dd>TRUE</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$max\\_depth] 6\n",
       "\\item[\\$eta] 0.05\n",
       "\\item[\\$objective] 'reg:squarederror'\n",
       "\\item[\\$validate\\_parameters] TRUE\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$max_depth\n",
       ":   6\n",
       "$eta\n",
       ":   0.05\n",
       "$objective\n",
       ":   'reg:squarederror'\n",
       "$validate_parameters\n",
       ":   TRUE\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$max_depth\n",
       "[1] 6\n",
       "\n",
       "$eta\n",
       "[1] 0.05\n",
       "\n",
       "$objective\n",
       "[1] \"reg:squarederror\"\n",
       "\n",
       "$validate_parameters\n",
       "[1] TRUE\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "21.348585"
      ],
      "text/latex": [
       "21.348585"
      ],
      "text/markdown": [
       "21.348585"
      ],
      "text/plain": [
       "[1] 21.34858"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max.depths = c(6,10,20)\n",
    "etas = c(0.3,0.1, 0.05)\n",
    "nrounds = c(100,200,400)\n",
    "\n",
    "best_params = 0\n",
    "best_score = 0\n",
    "\n",
    "count = 1\n",
    "\n",
    "for( depth in max.depths ) {\n",
    "    for(num in etas) {\n",
    "        for(numround in nrounds) {\n",
    "\n",
    "        bst_grid = xgb.train(data = dtrain, \n",
    "                                max.depth = depth, \n",
    "                                eta=num,  \n",
    "                                nround = numround, \n",
    "                                watchlist = watchlist, \n",
    "                                objective = \"reg:squarederror\", \n",
    "                                early_stopping_rounds = 50, \n",
    "                                verbose=0)\n",
    "\n",
    "        if(count == 1){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "            count = count + 1\n",
    "            }\n",
    "        else if( bst_grid$best_score < best_score){\n",
    "            best_params = bst_grid$params\n",
    "            best_score = bst_grid$best_score\n",
    "        }\n",
    "    }\n",
    " }\n",
    "}\n",
    "best_params\n",
    "best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use these parameters and get 50 validation errors for boosting (running for 50 seeds). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidymodels)\n",
    "\n",
    "boost.validation <- function(seed){\n",
    "    set.seed(seed)\n",
    "    len <- length(x[,1])\n",
    "    idx.train <- sample(1:len, 2*len/3)\n",
    "    train.x <- x[idx.train,]\n",
    "    train.y <- y[idx.train]\n",
    "    val.x <- x[-idx.train,]\n",
    "    val.y <- y[-idx.train]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(data =  as.matrix(train.x), label = train.y )\n",
    "    dval = xgb.DMatrix(data =  as.matrix(val.x), label = val.y)\n",
    "    \n",
    "    boost <- xgb.train(data = dtrain, \n",
    "                    objective = \"reg:squarederror\", \n",
    "                    max_depth = 6,\n",
    "                    eta=0.05,\n",
    "                    watchlist = watchlist,\n",
    "                    nround = 1000, \n",
    "                    verbose=0)\n",
    "    \n",
    "    prediction <- predict(boost, dval)\n",
    "    mean((prediction - val.y)^2)\n",
    "    \n",
    "}\n",
    "seeds <- seq.int(100,150)\n",
    "MSEs <- sapply(seeds, boost.validation)\n",
    "mean.MSE <- mean(sqrt(MSEs))\n",
    "var <- var(sqrt(MSEs))\n",
    "cat(\"The mean RMSE is = \", mean.MSE, \" and the variance is = \", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "23.6540402875021"
      ],
      "text/latex": [
       "23.6540402875021"
      ],
      "text/markdown": [
       "23.6540402875021"
      ],
      "text/plain": [
       "[1] 23.65404"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean.MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
