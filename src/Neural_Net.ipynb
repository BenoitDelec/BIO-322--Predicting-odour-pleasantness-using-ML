{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(keras)\n",
    "library(dplyr) \n",
    "use_condaenv('r-tensorflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function get.scale returns a list with two parameters : first the mean, second the std. If the input is already centered (it has an attribute center which is taken up by attr), we extract the mean. If it's not, we put the mean by default at 0. If the input is already scale (it has an attribute scale), we extract the std, if it's not we put the std by default at 1.\n",
    "The function scale.as enables to scale a data set X2 the same way X1 was scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.scale <- function(scaled) {\n",
    "  if (\"scaled:center\" %in% names(attributes(scaled))) {\n",
    "      center <- attr(scaled, \"scaled:center\")\n",
    "  } else {\n",
    "      center <- rep(0, ncol(scaled))\n",
    "  }\n",
    "  if (\"scaled:scale\" %in% names(attributes(scaled))) {\n",
    "      list(center, attr(scaled, \"scaled:scale\"))\n",
    "  } else {\n",
    "      list(center, rep(1., length(center)))\n",
    "  }\n",
    "}\n",
    "scale.as.x <- function(x, scaled) {\n",
    "  s <- get.scale(scaled)\n",
    "  centered <- sweep(x, 2, s[[1]])\n",
    "  sweep(centered, 2, s[[2]], FUN = \"/\")\n",
    "}\n",
    "\n",
    "scale.as.y <- function(y, scaled) {\n",
    "    s <- get.scale(scaled)\n",
    "    (y - s[[1]])/s[[2]]\n",
    "}\n",
    "\n",
    "unscale.y <- function(y, scaled) {\n",
    "    s <- get.scale(scaled)\n",
    "    y * s[[2]] + s[[1]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.data <- read.csv(file.path(\"..\", \"data\", \"training_data.csv\"))\n",
    "test.data <- read.csv(file.path(\"..\", \"data\", \"test_data.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for input variables with zero variance (without the varible Intensity)\n",
    "x = train.data[, -c(2,3)]\n",
    "y = train.data$VALENCE.PLEASANTNESS\n",
    "x$Intensity <- as.numeric(train.data$Intensity)\n",
    "idx.zero.var <- apply(x, 2, var) == 0\n",
    "#x is the whole training data (only predictors)\n",
    "x <- x[,!idx.zero.var]\n",
    "\n",
    "full.data = data.frame(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data between a training and a test set\n",
    "set.seed(100)\n",
    "len <- length(x[,1])\n",
    "idx.train <- sample(1:len, 2*len/3)\n",
    "\n",
    "train.x <- x[idx.train,]\n",
    "train.y <- y[idx.train]\n",
    "test.x <- x[-idx.train,]\n",
    "test.y <- y[-idx.train]\n",
    "\n",
    "train.x.prep <- scale(train.x, center = T, scale = T)\n",
    "train.y.prep <- scale(train.y, center = T, scale = T)\n",
    "\n",
    "test.x.scaled = scale.as.x(test.x,train.x.prep)\n",
    "test.y.scaled = scale.as.y(test.y,train.y.prep)\n",
    "#We have 472 rows in the training set, and 472 for the test set\n",
    "#Training data is scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn <- keras_model_sequential()\n",
    "nn <- nn %>%\n",
    "      layer_dense(units = 100, activation = 'relu', input_shape = c(dim(train.x)[2]), \n",
    "                  kernel_regularizer = regularizer_l2(l = 0.001)) %>%\n",
    "      #layer_dropout(rate=0.25)%>%\n",
    "      layer_dense(units = 100, activation = 'relu') %>%\n",
    "      #layer_dropout(rate=0.25)%>%\n",
    "      layer_dense(units = 100, activation = 'relu') %>%\n",
    "      layer_dense(units = 1, activation = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn %>% compile(optimizer = \"adam\", loss = \"mean_squared_error\", metrics = list(\"mean_absolute_error\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "history <- nn %>% fit(as.matrix(train.x.prep),\n",
    "                      as.matrix(train.y.prep),\n",
    "                      verbose = 0,\n",
    "                      batch_size = 100,\n",
    "                      validation_data = list(as.matrix(test.x.scaled),as.matrix(test.y.scaled)),\n",
    "                      epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "23.5781884951757"
      ],
      "text/latex": [
       "23.5781884951757"
      ],
      "text/markdown": [
       "23.5781884951757"
      ],
      "text/plain": [
       "[1] 23.57819"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.pred <- predict(nn, as.matrix(test.x.scaled))\n",
    "nn.pred.unscaled = unscale.y(nn.pred, train.y.prep)\n",
    "\n",
    "sqrt(mean((nn.pred.unscaled - test.y)^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
